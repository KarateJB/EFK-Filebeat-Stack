### Filebeat log ###

# input from beats
<source>
  @type beats
  tag "beats.collect"
  <parse>
    @type regexp
    expression /^(?<logtime>[^ ]* [^ ]*) \| (?<level>\w*) \| (?<class>\S*) \| (?<message>.*)/
    time_key logtime
    keep_time_key true
    time_key_format %Y-%m-%d %H:%M:%S.%L
    time_format %Y-%m-%d %H:%M:%S.%L
    timezone "+08:00"
    types line:integer
  </parse>
</source>

# See https://discuss.elastic.co/t/elasticsearch-not-able-to-identify-the-time-field-defined-in-fluentd/272594
# <filter *.**>
<filter beats.collect>
  @type record_transformer
  enable_ruby
  auto_typecast true
  <record>
    # logdatetime ${require 'date'; DateTime.parse(record["logtime"].to_s + " +0800").iso8601(4).to_s}
    logdatetime ${require 'time'; Time.parse(record["logtime"].to_s + " +0800").iso8601(4).to_s}
  </record>
</filter>

# Inject custom fields into event
# See https://docs.fluentd.org/how-to-guides/filter-modify-apache#how-to-inject-custom-fields-into-events
# Set new tag as fields.tag
# The following fields setting from filebeat.yml, you can check this file from Filbeat Folder
<match beats.collect>
  @type record_reformer
  tag ${fields['tag']}
  hostname ${fields['hostname']}
  inventory_hostname ${fields['inventory_hostname']}
  ip ${fields['ip']}
  time_key ${fields['logtime']}
  remove_keys $.host, $.agent, $.fields, $.ecs, $.@metadata 
</match>

# Filter for beat log
<filter filebeat>
  @type parser
  key_name message
  reserve_data true
  remove_key_name_field true
  <parse>
    @type regexp
    expression  /(?<time>[^\t]*)\t(?<level>[^\t]*)(\t\[(?<logger_name>[^\t]*)\])?\t(?<logger_caller>[^\t]*)\t(?<message>[^\t]*)(\t(?<json_data>[\w\W]*))?/
    time_format %Y-%m-%dT%H:%M:%S.%L%z
    timezone "+08:00"
    reserve_time true
  </parse>
</filter>

# Output
<match filebeat>
@type copy
  <store>
    @log_level debug
    @type elasticsearch
    host logmon-elasticsearch
    port 9200
    type_name logs
    logstash_format true
    include_tag_key true
    utc_index true
    <buffer>
      @type file
      path /var/log/fluentd-buffers/log_monitor.buffer
      flush_mode interval
      retry_type exponential_backoff
      flush_thread_count 4
      flush_interval 5s
      retry_forever
      retry_max_interval 30
      chunk_limit_size "2M"
      queue_limit_length "8"
      overflow_action block
    </buffer>
  </store>
  <store>
    @type stdout
  </store>
</match>


### AP: Auth container's log ###

# Use the commented out source when fluentd can get the mounted log files
# <source>
#   @type tail
#   path /var/log/filebeat/Auth/**/*
#   tag ap.auth
#   pos_file /var/log/filebeat/Auth.pos

#   <parse>
#     @type regexp
#     expression /(?<time>[^\]]*) \[(.*)\] \[(?<level>\w*)\] \[(?<class>\S*)\] (?<message>.*)/
#     time_format %Y-%m-%d %H:%M:%S.%L
#     timezone "+08:00"
#     types line:integer
#   </parse>
# </source>

<match ap.auth>

  @type copy

  <store>
    @log_level debug
    @type elasticsearch
    host logmon-elasticsearch
    port 9200
    type_name logs
    logstash_format true
    include_tag_key true
    utc_index true
    flush_interval 1s
  </store>

  <store>
    @type stdout
  </store>

</match>

### AP: Backend container's log ###

# Use the commented out source when fluentd can get the mounted log files
# <source>
#   @type tail
#   path /var/log/filebeat/Backend/**/*
#   tag ap.auth
#   pos_file /var/log/filebeat/Backend.pos

#   <parse>
#     @type regexp
#     expression /(?<time>[^\]]*) \[(.*)\] \[(?<level>\w*)\] \[(?<class>\S*)\] (?<message>.*)/
#     time_format %Y-%m-%d %H:%M:%S.%L
#     timezone "+08:00"
#     types line:integer
#   </parse>
# </source>


<match ap.backend>

  @type copy

  <store>
    @log_level debug
    @type elasticsearch
    host logmon-elasticsearch
    port 9200
    type_name logs
    logstash_format true
    include_tag_key true
    utc_index true
    flush_interval 1s
  </store>

  <store>
    @type stdout
  </store>

</match>
